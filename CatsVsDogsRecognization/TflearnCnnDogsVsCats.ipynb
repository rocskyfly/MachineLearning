{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "数据集说明：数据集来自Kaggle猫狗识别比赛\n",
    "训练集共有25000张已被标注的大小不尽相同的猫和狗的图片，猫狗图片数量各一半，图片命名规则为:[dog|cat].index.jpg\n",
    "测试集共有12500张未被标注的大小不尽相同的猫和狗的图片。图片命名规则为:index.jpg\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置图片大小：长度和宽度\n",
    "IMG_LENGTH = 50\n",
    "IMG_WIDTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_img(img):\n",
    "    \"\"\"\n",
    "    获取训练集每张图片的标签。\n",
    "    :param img: 输入为图片文件名\n",
    "    :return: 输出为标签，猫为[1,0]，狗为[0,1]\n",
    "    \"\"\"\n",
    "    word_label = img.split('.')[-3]\n",
    "\n",
    "    if word_label == 'cat':\n",
    "        return [1, 0]\n",
    "    elif word_label == 'dog':\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset(dest, img_length, img_width, train_dataset_name):\n",
    "    \"\"\"\n",
    "    训练集数据预处理，获取训练集加标签的数据。\n",
    "    :param dest: 训练集原始数据所在的目录\n",
    "    :param img_length: 重新设置图片的长度\n",
    "    :param img_width: 重新设置图片的宽度\n",
    "    :param train_dataset_name: 保存处理后的数据集的名称\n",
    "    :return: 训练集数据list，单个元素为[np.array(img), np.array(label)]\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    for img in tqdm(os.listdir(dest)):  # tqdm进度条，用户只需要封装任意的迭代器tqdm(iterator)\n",
    "        label = label_img(img)\n",
    "        path = os.path.join(dest, img)\n",
    "\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_std = cv2.resize(img, (img_length, img_width))\n",
    "\n",
    "        training_data.append([np.array(img_std), np.array(label)])\n",
    "\n",
    "    # 将序列的所有元素随机排序\n",
    "    shuffle(training_data)\n",
    "\n",
    "    # 将training_data保存为文件，以备下次使用\n",
    "    np.save(train_dataset_name, training_data)\n",
    "\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset(dest, img_length, img_width, test_dataset_name):\n",
    "    \"\"\"\n",
    "    测试集数据预处理，获取测试集数据。\n",
    "    :param dest: 训练集原始数据所在的目录\n",
    "    :param img_length: 重新设置图片的长度\n",
    "    :param img_width: 重新设置图片的宽度\n",
    "    :param test_dataset_name: 保存处理后的数据集的名称\n",
    "    :return: 测试集数据list，单个元素为[np.array(img), img_num]\n",
    "    \"\"\"\n",
    "    testing_data = []\n",
    "    for img in tqdm(os.listdir(dest)):\n",
    "        img_num = img.split('.')[0]\n",
    "        path = os.path.join(dest, img)\n",
    "\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_std = cv2.resize(img, (img_length, img_width))\n",
    "\n",
    "        testing_data.append([np.array(img_std), img_num])\n",
    "\n",
    "    # 将testing_data保存为文件，以备下次使用\n",
    "    np.save(test_dataset_name, testing_data)\n",
    "\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preprocessing(train_dir, test_dir, img_length=IMG_LENGTH, img_width=IMG_WIDTH):\n",
    "    \"\"\"\n",
    "    获取并预处理训练集和测试集数据\n",
    "    \"\"\"\n",
    "    train_dataset_name = 'train_dataset-{}x{}.npy'.format(img_length, img_width)\n",
    "    test_dataset_name = 'test_dataset-{}x{}.npy'.format(img_length, img_width)\n",
    "\n",
    "    if os.path.exists(train_dataset_name):\n",
    "        train_dataset = np.load(train_dataset_name)\n",
    "    else:\n",
    "        train_dataset = create_train_dataset(train_dir, img_length, img_width, train_dataset_name)\n",
    "\n",
    "    if os.path.exists(test_dataset_name):\n",
    "        test_dataset = np.load(test_dataset_name)\n",
    "    else:\n",
    "        test_dataset = create_test_dataset(test_dir, img_length, img_width, test_dataset_name)\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    \"\"\"\n",
    "    定义卷积神经网络模型\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classifiers=2, lr=1e-3, img_length=IMG_LENGTH, img_width=IMG_WIDTH):\n",
    "        self.classifiers = classifiers\n",
    "        self.lr = lr\n",
    "        self.img_length = img_length\n",
    "        self.img_width = img_width\n",
    "        self.log_dir = 'log'\n",
    "        self.model_name = 'dogsvscats-{}-{}.model'.format(lr, '6conv-basic')\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        convnet = input_data(shape=[None, self.img_length, self.img_width, 1], name='input')\n",
    "\n",
    "        convnet = conv_2d(convnet, nb_filter=32, filter_size=5, activation='relu')\n",
    "        convnet = max_pool_2d(convnet, kernel_size=5)\n",
    "\n",
    "        convnet = conv_2d(convnet, nb_filter=64, filter_size=5, activation='relu')\n",
    "        convnet = max_pool_2d(convnet, kernel_size=5)\n",
    "\n",
    "        convnet = conv_2d(convnet, nb_filter=32, filter_size=2, activation='relu')\n",
    "        convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "        convnet = conv_2d(convnet, nb_filter=64, filter_size=2, activation='relu')\n",
    "        convnet = max_pool_2d(convnet, kernel_size=2)\n",
    "\n",
    "        convnet = conv_2d(convnet, nb_filter=32, filter_size=2, activation='relu')\n",
    "        convnet = max_pool_2d(convnet, kernel_size=2)\n",
    "\n",
    "        convnet = conv_2d(convnet, nb_filter=64, filter_size=2, activation='relu')\n",
    "        convnet = max_pool_2d(convnet, kernel_size=2)\n",
    "\n",
    "        convnet = fully_connected(convnet, n_units=1024, activation='relu')\n",
    "\n",
    "        convnet = dropout(convnet, keep_prob=0.5)\n",
    "\n",
    "        convnet = fully_connected(convnet, classifiers, activation='softmax')\n",
    "        convnet = regression(convnet, optimizer='adam', learning_rate=self.lr, loss='categorical_crossentropy',\n",
    "                             name='targets')\n",
    "        # convnet = regression(convnet, optimizer='adam', learning_rate=learn_rate, loss='binary_crossentropy',\n",
    "        #                      name='targets')\n",
    "\n",
    "        # 指定tensorboard_dir，可以将运行中生成的结构化数据放在此目录下，为tensorboard可视化提供数据,命令：\n",
    "        # tensorboard --logdir full path of tensorboard_dir\n",
    "        self.network = tflearn.DNN(convnet, tensorboard_dir=self.log_dir)\n",
    "\n",
    "    def fit(self, train_dataset):\n",
    "        # split out training and testing data，有标签数据分为训练集和测试集\n",
    "        train = train_dataset[:-2500]\n",
    "        test = train_dataset[-2500:]\n",
    "\n",
    "        # separate my features and labels　特征，类别分离\n",
    "        x = np.array([i[0] for i in train]).reshape(-1, self.img_length, self.img_width, 1)\n",
    "        y = [i[1] for i in train]\n",
    "        # print \"x shape is:%s\\t y size is:%s\" % (x.shape, len(y))\n",
    "\n",
    "        test_x = np.array([i[0] for i in test]).reshape(-1, self.img_length, self.img_width, 1)\n",
    "        test_y = [i[1] for i in test]\n",
    "        # print \"test x shape is:%s\\t test y size is:%s\" % (test_x.shape, len(test_y))\n",
    "\n",
    "        # run_id for tensorboard\n",
    "        self.network.fit({'input': x}, {'targets': y}, n_epoch=5,\n",
    "                         validation_set=({'input': test_x}, {'targets': test_y}),\n",
    "                         snapshot_step=500, show_metric=True, run_id=self.model_name)\n",
    "\n",
    "        return self.network\n",
    "\n",
    "    def save(self):\n",
    "        self.network.save(self.model_name)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def load(self):\n",
    "        if os.path.exists('{}.meta'.format(self.model_name)):\n",
    "            self.network.load(self.model_name)\n",
    "            print \"model loaded!\"\n",
    "            return self.network\n",
    "        else:\n",
    "            print \"no existed model, need training\"\n",
    "            return False\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        if 0 < len(dataset) <= 12:\n",
    "            fig = plt.figure()\n",
    "\n",
    "            # predict first 12 data in test data\n",
    "            for num, data in enumerate(dataset):  # enumerate将其组成一个索引序列，利用它可以同时获得索引和值\n",
    "                img_data = data[0]\n",
    "                img_index = data[1]\n",
    "\n",
    "                # 将画布分割成3行4列，图像画在从左到右从上到下的第num+1块\n",
    "                y = fig.add_subplot(3, 4, num + 1)\n",
    "                orig = img_data\n",
    "                data = img_data.reshape(self.img_length, self.img_width, 1)\n",
    "\n",
    "                model_out = self.network.predict([data])[0]\n",
    "\n",
    "                if np.argmax(model_out) == 1:\n",
    "                    str_label = 'Dog-' + img_index\n",
    "                else:\n",
    "                    str_label = 'Cat-' + img_index\n",
    "\n",
    "                y.imshow(orig, cmap='gray')\n",
    "                plt.title(str_label)\n",
    "                y.axes.get_xaxis().set_visible(False)\n",
    "                y.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "            plt.show()\n",
    "        else:\n",
    "            with open('result.csv', 'w') as f:\n",
    "                f.write('id,label\\n')\n",
    "\n",
    "            with open('result.csv', 'a') as f:\n",
    "                for data in tqdm(dataset):\n",
    "                    img_num = data[1]\n",
    "                    img_data = data[0]\n",
    "                    data = img_data.reshape(self.img_length, self.img_width, 1)\n",
    "                    model_out = self.network.predict([data])[0]\n",
    "\n",
    "                    f.write('{},{}\\n'.format(img_num, model_out[1]))\n",
    "            print \"Result show in file result.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_dir_name = './train'\n",
    "    test_dir_name = './test'\n",
    "\n",
    "    # 获取训练数据集及测试数据集\n",
    "    train_data, test_data = dataset_preprocessing(train_dir_name, test_dir_name)\n",
    "\n",
    "    model = CNN()\n",
    "\n",
    "    if not model.load():\n",
    "        model.fit(train_data)\n",
    "        model.save()\n",
    "\n",
    "    model.predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
